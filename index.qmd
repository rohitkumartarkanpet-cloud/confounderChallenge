
---
title: "Using Randomization & Stratification to Overcome a Common Cause Confounder"
subtitle: "Comparing Surgical Outcomes for Doc Dreamy and Doc Duck"
format:
  html:
    toc: true
    toc-depth: 2
    number-sections: true
    fig-align: center
    fig-width: 7
    fig-height: 4
execute:
  echo: false
  eval: true
  warning: false
  message: false
---

```{=html}
<style>
.quarto-figure img, img {
  max-width: 100%;
  height: auto;
}
</style>
```

## Introduction

Clinical data often look convincing at first glance. A simple comparison of averages can make one treatment or clinician appear superior. However, when treatment choice is related to patient characteristics, those same averages can be badly misleading.

In this report, we study outcomes for two surgeons working at the same hospital:

- **Doc Dreamy**, who fits the stereotypical image of a highly polished specialist.
- **Doc Duck**, whose appearance and style may not match common expectations.

```{=html}
<div style="display:flex; justify-content:center; margin: 1rem 0 1.5rem 0;">
  <img src="docSideBySide.jpg"
       alt="Doc Dreamy and Doc Duck"
       style="max-width: 340px; border-radius: 10px; box-shadow: 0 0 18px rgba(0,0,0,0.25);">
</div>
```

We use observational data, randomized data, and stratification by baseline severity to show how a common-cause confounder can reverse the apparent ranking of these two surgeons. Randomization and stratification are then used to recover a more credible causal story.

## Data and Notation

```{python}
#| label: load-data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load observational and randomized data from CSV
patients_df = pd.read_csv("patients_data.csv")
patients_randomized_df = pd.read_csv("patients_data_randomized.csv")

# Color choices for light-style figures
dreamy_color = "#4F46E5"  # indigo
duck_color   = "#16A34A"  # green
grid_color   = "#D1D5DB"  # light gray grid

n_patients_obs = len(patients_df)
n_patients_rnd = len(patients_randomized_df)
```

We work with two data sets:

- **Observational data**: patients selected their surgeon under usual care (driven by preferences, urgency, appearance, scheduling, and so on).
- **Randomized data**: patients were assigned to a surgeon using a random mechanism, independent of patient characteristics.

For each patient, we observe:

- `severity` – baseline symptom severity before surgery.
- `doctor_name` – `"Doc Dreamy"` or `"Doc Duck"`.
- `post_surgical_score` – symptom score after surgery  
  (lower values indicate better outcomes).

Throughout, the goal is not only to describe these data, but to understand **how much of the observed difference in outcomes can be attributed to the choice of surgeon**, rather than to differences in the types of patients they treat.

## Observational Data: A First, Misleading Impression

In the observational dataset, patients did not receive surgeons at random. They chose, or were scheduled with, a surgeon based on real-world factors such as availability, perceived skill, and appearance. As a result, baseline severity can be imbalanced between the two surgeons.

### Summary of Observational Outcomes

```{python}
#| label: tbl-observational-summary

summary_obs = (
    patients_df
    .groupby("doctor_name")
    .agg(
        n=("post_surgical_score", "size"),
        mean_score=("post_surgical_score", "mean"),
        std_score=("post_surgical_score", "std"),
        mean_severity=("severity", "mean"),
    )
    .reset_index()
)

def style_summary(df, caption):
    styler = (
        df.style
        .set_caption(caption)
        .format({
            "mean_score": "{:.2f}",
            "std_score": "{:.2f}",
            "mean_severity": "{:.2f}",
        })
        .set_table_attributes('class="table table-striped table-bordered"')
        .set_properties(**{"text-align": "center"})
        .set_table_styles([
            {"selector": "caption",
             "props": [("caption-side", "top"),
                       ("font-weight", "bold"),
                       ("padding", "6px"),
                       ("color", "#111827")]},
            {"selector": "th",
             "props": [("background-color", "#F3F4F6"),
                       ("color", "#111827"),
                       ("padding", "8px")]},
            {"selector": "td",
             "props": [("padding", "6px"),
                       ("color", "#111827"),
                       ("border-top", "1px solid #E5E7EB")]},
        ])
    )
    return styler

style_summary(
    summary_obs,
    "Table 1. Summary of observational outcomes by surgeon."
)
```

In the observational data, Doc Dreamy has a lower average post-surgical symptom score than Doc Duck. If one simply compares these averages, Doc Dreamy appears to be the better surgeon. However, the same summary also shows that Doc Dreamy tends to treat less severe cases on average, while Doc Duck treats more severe cases. This raises the possibility that the apparent advantage for Doc Dreamy is driven by differences in the patients, not differences in skill.

### Observational Outcomes by Patient

```{python}
#| label: fig-observational
#| fig-cap: "Observed post-surgical symptom scores for each patient (left), and mean scores with 95% confidence intervals (right)."
#| out-width: "100%"
#| fig-align: "center"

dreamy = patients_df[patients_df["doctor_name"] == "Doc Dreamy"]
duck   = patients_df[patients_df["doctor_name"] == "Doc Duck"]

scores = [dreamy["post_surgical_score"], duck["post_surgical_score"]]
labels = ["Doc Dreamy", "Doc Duck"]
colors = [dreamy_color, duck_color]

fig, axes = plt.subplots(1, 2, figsize=(9, 4), gridspec_kw={"width_ratios": [3, 2]})
ax_scatter, ax_bar = axes

# --- Left: jittered patients on a vertical strip ---
rng = np.random.default_rng(2027)
positions = [0, 1]

for pos, sers, color in zip(positions, scores, colors):
    x_jitter = pos + rng.normal(0, 0.06, size=len(sers))
    ax_scatter.scatter(
        x_jitter,
        sers,
        s=30,
        color=color,
        alpha=0.75,
        edgecolor="#374151",
        linewidth=0.3,
    )

ax_scatter.set_xticks(positions)
ax_scatter.set_xticklabels(labels, rotation=0)
ax_scatter.set_ylabel("Post-surgical symptom score (lower is better)")
ax_scatter.set_title("Individual patients")

ax_scatter.grid(True, linestyle=":", color=grid_color, alpha=0.8, axis="y")
ax_scatter.set_xlim(-0.4, 1.4)

# --- Right: mean ± 95% CI bar chart ---
means = [s.mean() for s in scores]
ses = [s.std(ddof=1) / np.sqrt(len(s)) for s in scores]
cis = [1.96 * se for se in ses]

x_bar = np.arange(len(labels))
ax_bar.bar(x_bar, means, color=colors, alpha=0.8)
ax_bar.errorbar(
    x_bar,
    means,
    yerr=cis,
    fmt="none",
    ecolor="#111827",
    elinewidth=1.4,
    capsize=6,
)
for x, m in zip(x_bar, means):
    ax_bar.text(x, m + 0.05, f"{m:.2f}", ha="center", va="bottom", fontsize=9)

ax_bar.set_xticks(x_bar)
ax_bar.set_xticklabels(labels, rotation=0)
ax_bar.set_title("Mean score with 95% CI")
ax_bar.grid(True, linestyle=":", color=grid_color, alpha=0.8, axis="y")

plt.tight_layout()
plt.show()
```

From the left panel we see the full spread of outcomes for each surgeon. The right panel summarizes these outcomes numerically, making the size and uncertainty of the difference in means easy to read without crowding the raw data.

## Severity as a Common-Cause Confounder

A useful way to structure this problem is through directed acyclic graphs (DAGs). A simple mental model is shown in Figure 2.

![Simple model: surgeon directly affects outcome.](dag-example-node.png){#fig-example-node width=55%}

In this initial picture, surgeon identity directly affects the post-surgical outcome. If we believe this model and take the observational difference in mean symptom scores at face value, we might conclude that Doc Dreamy is genuinely better.

However, suppose baseline severity influences both **which surgeon a patient sees** and **how well that patient recovers**. In that case, severity becomes a common-cause confounder, as illustrated in Figure 3.

![Common-cause model: severity influences both surgeon choice and outcome.](dag-example2-node.png){#fig-example2-node width=55%}

A plausible story is the following:

- Patients with **milder symptoms** may be willing to wait longer and may prefer the surgeon who appears more polished and reassuring (Doc Dreamy).
- Patients with **more severe symptoms** may be motivated to seek the earliest available surgery date, even if that means seeing a surgeon who does not fit the stereotypical image (Doc Duck).

If this is true, then Doc Dreamy will systematically see easier cases and Doc Duck will systematically see harder cases. In this setting, the raw comparison of mean outcomes is biased in favor of Doc Dreamy, even if Doc Duck is actually the more skilled surgeon.

To recover a credible causal comparison, we need a design that breaks the link between severity and surgeon choice, or a statistical strategy that accounts for severity directly.

## Randomized Assignment: Breaking the Confounding Path

Randomized experiments are designed to address precisely this type of problem. Under randomization, patients no longer choose their surgeon based on appearance, availability, or severity. Instead, a random mechanism (like a coin flip) assigns each patient to a surgeon, making surgeon identity independent of baseline severity.

This design is summarized conceptually in Figure 4.

![Randomization breaks the link between severity and surgeon choice.](dag-example3-node.png){#fig-example3-node width=65%}

In this DAG, severity still affects the outcome, but it no longer affects surgeon choice. Randomization determines the surgeon, which allows us to interpret differences in outcomes as the causal effect of surgeon identity, up to random variation.

### Summary of Randomized Outcomes

```{python}
#| label: tbl-randomized-summary

summary_rnd = (
    patients_randomized_df
    .groupby("doctor_name")
    .agg(
        n=("post_surgical_score", "size"),
        mean_score=("post_surgical_score", "mean"),
        std_score=("post_surgical_score", "std"),
        mean_severity=("severity", "mean"),
    )
    .reset_index()
)

style_summary(
    summary_rnd,
    "Table 2. Summary of randomized outcomes by surgeon."
)
```

The randomized summary reveals a very different pattern from the observational data. In this setting:

- Doc Duck has a **lower average post-surgical score** than Doc Dreamy.
- The average baseline severity is similar across the two surgeons, reflecting the fact that assignment was not driven by severity.

Because surgeon identity is no longer entangled with baseline severity, this difference in mean outcomes is much closer to the causal effect of interest.

### Randomized Outcomes by Patient

```{python}
#| label: fig-randomized
#| fig-cap: "Post-surgical symptom scores under randomized surgeon assignment (left: individual patients; right: mean scores with 95% confidence intervals)."
#| out-width: "100%"
#| fig-align: "center"

dreamy_r = patients_randomized_df[patients_randomized_df["doctor_name"] == "Doc Dreamy"]
duck_r   = patients_randomized_df[patients_randomized_df["doctor_name"] == "Doc Duck"]

scores_r = [dreamy_r["post_surgical_score"], duck_r["post_surgical_score"]]
labels_r = ["Doc Dreamy", "Doc Duck"]
colors_r = [dreamy_color, duck_color]

fig, axes = plt.subplots(1, 2, figsize=(9, 4), gridspec_kw={"width_ratios": [3, 2]})
ax_scatter_r, ax_bar_r = axes

rng_r = np.random.default_rng(2028)
positions_r = [0, 1]

for pos, sers, color in zip(positions_r, scores_r, colors_r):
    x_jitter = pos + rng_r.normal(0, 0.06, size=len(sers))
    ax_scatter_r.scatter(
        x_jitter,
        sers,
        s=30,
        color=color,
        alpha=0.75,
        edgecolor="#374151",
        linewidth=0.3,
    )

ax_scatter_r.set_xticks(positions_r)
ax_scatter_r.set_xticklabels(labels_r)
ax_scatter_r.set_ylabel("Post-surgical symptom score (lower is better)")
ax_scatter_r.set_title("Individual patients")

ax_scatter_r.grid(True, linestyle=":", color=grid_color, alpha=0.8, axis="y")
ax_scatter_r.set_xlim(-0.4, 1.4)

means_r = [s.mean() for s in scores_r]
ses_r = [s.std(ddof=1) / np.sqrt(len(s)) for s in scores_r]
cis_r = [1.96 * se for se in ses_r]

x_bar_r = np.arange(len(labels_r))
ax_bar_r.bar(x_bar_r, means_r, color=colors_r, alpha=0.8)
ax_bar_r.errorbar(
    x_bar_r,
    means_r,
    yerr=cis_r,
    fmt="none",
    ecolor="#111827",
    elinewidth=1.4,
    capsize=6,
)
for x, m in zip(x_bar_r, means_r):
    ax_bar_r.text(x, m + 0.05, f"{m:.2f}", ha="center", va="bottom", fontsize=9)

ax_bar_r.set_xticks(x_bar_r)
ax_bar_r.set_xticklabels(labels_r)
ax_bar_r.set_title("Mean score with 95% CI")
ax_bar_r.grid(True, linestyle=":", color=grid_color, alpha=0.8, axis="y")

plt.tight_layout()
plt.show()
```

In the randomized data, the right-hand panel makes clear that Doc Duck’s mean lies below Doc Dreamy’s, with overlapping but shifted confidence intervals—evidence that Duck is the more effective surgeon on average when case mix is balanced by design.

Randomization allows us to ask a clean question:

> *If two similar groups of patients are assigned to surgeons by chance, which surgeon tends to achieve better outcomes?*

In these data, the randomized comparison points to Doc Duck.

## Stratification by Severity: When Randomization Is Not Available

Randomized experiments are powerful, but they are not always feasible. In many real-world settings, we must work with observational data only. In those cases, one common strategy is **stratification**: comparing outcomes within groups of patients who share similar values of the confounder.

Here, the confounder is baseline severity. We therefore examine post-surgical scores as a function of initial severity, using the observational data.

### Outcomes by Baseline Severity

```{python}
#| label: fig-severity
#| fig-cap: "Post-surgical symptom scores versus baseline severity in the observational data, with smoothed trend lines for each surgeon. The shaded band marks the overlap region (-1 ≤ severity ≤ 1)."
#| out-width: "100%"
#| fig-align: "center"

fig, ax = plt.subplots(figsize=(7, 4))

dreamy = patients_df[patients_df["doctor_name"] == "Doc Dreamy"]
duck   = patients_df[patients_df["doctor_name"] == "Doc Duck"]

# Overlap band
ax.axvspan(-1, 1, color="#E5E7EB", alpha=0.8, label="Overlap region (-1 to 1)")

ax.scatter(
    dreamy["severity"],
    dreamy["post_surgical_score"],
    s=28,
    color=dreamy_color,
    alpha=0.75,
    edgecolor="#374151",
    linewidth=0.3,
    label="Doc Dreamy patients",
)
ax.scatter(
    duck["severity"],
    duck["post_surgical_score"],
    s=28,
    color=duck_color,
    alpha=0.75,
    edgecolor="#374151",
    linewidth=0.3,
    label="Doc Duck patients",
)

# Simple smoothed trends via polynomial fit (degree 2)
for data, color, name in [(dreamy, dreamy_color, "Doc Dreamy trend"),
                          (duck, duck_color, "Doc Duck trend")]:
    x = data["severity"].values
    y = data["post_surgical_score"].values
    x_grid = np.linspace(x.min(), x.max(), 200)
    coeffs = np.polyfit(x, y, deg=2)
    y_smooth = np.polyval(coeffs, x_grid)
    ax.plot(x_grid, y_smooth, color=color, linewidth=2.0, label=name)

ax.set_xlabel("Baseline severity")
ax.set_ylabel("Post-surgical symptom score (lower is better)")
ax.set_title("Post-surgical symptom scores by baseline severity – observational data")

ax.grid(True, linestyle=":", color=grid_color, alpha=0.9)
ax.legend(frameon=False, loc="upper left")

plt.tight_layout()
plt.show()
```

In Figure 3, the shaded vertical band again marks the severity range where both surgeons treat many patients. The smooth trend lines help show how outcomes vary with baseline severity for each surgeon, without cluttering the figure with numerical labels.

### Summary Within the Overlap Band

```{python}
#| label: tbl-overlap-summary

overlap = patients_df[(patients_df["severity"] >= -1) & (patients_df["severity"] <= 1)]

summary_overlap = (
    overlap
    .groupby("doctor_name")
    .agg(
        n=("post_surgical_score", "size"),
        mean_score=("post_surgical_score", "mean"),
        std_score=("post_surgical_score", "std"),
        mean_severity=("severity", "mean"),
    )
    .reset_index()
)

style_summary(
    summary_overlap,
    "Table 3. Summary of outcomes within the overlap band (-1 ≤ severity ≤ 1)."
)
```

The overlap summary focuses on patients whose baseline severity lies between –1 and 1, where the two surgeons are directly comparable. Within this band, the difference in average post-surgical scores is smaller than in the full observational sample, and Doc Duck often appears more competitive or even favorable relative to Doc Dreamy.

This stratified comparison reduces the bias introduced by severity. While it does not replicate the clean separation achieved by randomization, it moves the analysis closer to an apples-to-apples comparison.

## Discussion and Main Lessons

Taken together, the observational analysis, randomized experiment, and stratified comparison tell a consistent story about confounding and causal interpretation.

1. **Raw observational averages can be misleading.**  
   In the observational data, Doc Dreamy initially appears to be the better surgeon because his patients have lower average post-surgical scores. However, he also tends to see less severe cases, while Doc Duck treats more severe cases. The naive comparison of means therefore conflates surgeon skill with differences in case mix.

2. **Randomization breaks the link between severity and treatment.**  
   In the randomized data, surgeon assignment is no longer determined by factors such as severity or perceived appearance. Under this design, Doc Duck achieves lower average post-surgical scores than Doc Dreamy, suggesting that he is the more effective surgeon when patients are comparable at baseline.

3. **Stratification is a practical alternative when randomization is not available.**  
   When only observational data are available, stratifying on baseline severity helps to control for the confounder. Comparing outcomes within bands of severity (such as the overlap region between –1 and 1) produces a fairer comparison and reduces the bias induced by non-random patient allocation.

The broader message is that **causal interpretation requires more than a simple comparison of averages**. Understanding the data-generating process—who sees which surgeon and why—is essential. Randomization, when feasible, is a powerful design tool for breaking confounding pathways. When randomization is not possible, careful adjustment strategies such as stratification can still provide meaningful insight, though with stronger assumptions.

In this example, the combination of randomized evidence and stratified observational analysis points to a clear conclusion: despite appearances and initial impressions from the raw observational data, Doc Duck is likely the better surgeon. The apparent advantage of Doc Dreamy in the observational analysis is largely explained by severity acting as a common-cause confounder.
