---
title: "Using Randomization & Stratification to Overcome a Common Cause Confounder"
subtitle: "Comparing Surgical Outcomes for Doc Dreamy and Doc Duck"
format:
  html:
    toc: true
    toc-depth: 2
    number-sections: true
    fig-align: center
    fig-width: 7
    fig-height: 4
execute:
  echo: false
  eval: true
  warning: false
  message: false
---

```{=html}
<style>
.quarto-figure img, img {
  max-width: 100%;
  height: auto;
}
</style>
```

## Introduction

Clinical data often look convincing at first glance. A simple comparison of averages can make one treatment or clinician appear superior. However, when treatment choice is related to patient characteristics, those same averages can be badly misleading.

In this report, we study outcomes for two surgeons working at the same hospital:

- **Doc Dreamy**, who fits the stereotypical image of a highly polished specialist.
- **Doc Duck**, whose appearance and style may not match common expectations.

We use observational data, randomized data, and stratification by baseline severity to show how a common-cause confounder can reverse the apparent ranking of these two surgeons. Randomization and stratification are then used to recover a more credible causal story.

## Data and Notation

```{python}
#| label: load-data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load observational and randomized data from CSV
patients_df = pd.read_csv("patients_data.csv")
patients_randomized_df = pd.read_csv("patients_data_randomized.csv")

# Color choices and background for dark-style figures
dreamy_color = "#7C3AED"  # violet
duck_color   = "#22C55E"  # green
bg_color     = "#020617"  # very dark background
grid_color   = "#4B5563"  # gray grid

n_patients_obs = len(patients_df)
n_patients_rnd = len(patients_randomized_df)
```

We work with two data sets:

- **Observational data**: patients selected their surgeon under usual care (driven by preferences, urgency, appearance, scheduling, and so on).
- **Randomized data**: patients were assigned to a surgeon using a random mechanism, independent of patient characteristics.

For each patient, we observe:

- `severity` – baseline symptom severity before surgery.
- `doctor_name` – `"Doc Dreamy"` or `"Doc Duck"`.
- `post_surgical_score` – symptom score after surgery  
  (lower values indicate better outcomes).

Throughout, the goal is not only to describe these data, but to understand **how much of the observed difference in outcomes can be attributed to the choice of surgeon**, rather than to differences in the types of patients they treat.

## Observational Data: A First, Misleading Impression

In the observational dataset, patients did not receive surgeons at random. They chose, or were scheduled with, a surgeon based on real-world factors such as availability, perceived skill, and appearance. As a result, baseline severity can be imbalanced between the two surgeons.

### Summary of Observational Outcomes

```{python}
#| label: tbl-observational-summary

summary_obs = (
    patients_df
    .groupby("doctor_name")
    .agg(
        n=("post_surgical_score", "size"),
        mean_score=("post_surgical_score", "mean"),
        std_score=("post_surgical_score", "std"),
        mean_severity=("severity", "mean"),
    )
    .reset_index()
)

def style_summary(df):
    return (
        df.style
        .format({
            "mean_score": "{:.2f}",
            "std_score": "{:.2f}",
            "mean_severity": "{:.2f}",
        })
        .set_table_attributes('class="table table-striped table-bordered"')
        .set_properties(**{"text-align": "center"})
        .set_table_styles([
            {"selector": "th",
             "props": [("background-color", "#111827"),
                       ("color", "white"),
                       ("padding", "8px")]},
            {"selector": "td",
             "props": [("padding", "6px")]}
        ])
        .background_gradient(axis=0, cmap="plasma", subset=["mean_score"])
    )

style_summary(summary_obs)
```

In the observational data, Doc Dreamy has a lower average post-surgical symptom score than Doc Duck. If one simply compares these averages, Doc Dreamy appears to be the better surgeon. However, the same summary also shows that Doc Dreamy tends to treat less severe cases on average, while Doc Duck treats more severe cases. This raises the possibility that the apparent advantage for Doc Dreamy is driven by differences in the patients, not differences in skill.

### Figure&nbsp;1 – Observational Outcomes by Patient

```{python}
#| label: fig-observational
#| fig-cap: "Observed post-surgical symptom scores for each patient, by surgeon."
#| out-width: "100%"
#| fig-align: "center"

fig, ax = plt.subplots(figsize=(7, 4), facecolor=bg_color)
ax.set_facecolor(bg_color)

dreamy = patients_df[patients_df["doctor_name"] == "Doc Dreamy"]
duck   = patients_df[patients_df["doctor_name"] == "Doc Duck"]

# Slight jitter on x so overlapping markers are easier to see
rng = np.random.default_rng(123)
jitter_dreamy = rng.uniform(-0.15, 0.15, size=len(dreamy))
jitter_duck   = rng.uniform(-0.15, 0.15, size=len(duck))

# Scatter points with a subtle glow effect
for size, alpha in [(70, 0.9), (120, 0.15)]:
    ax.scatter(
        dreamy["patient"] + jitter_dreamy,
        dreamy["post_surgical_score"],
        s=size,
        marker="o",
        edgecolor="white",
        linewidth=0.5,
        color=dreamy_color,
        alpha=alpha,
        label="Doc Dreamy" if size == 70 else None,
        zorder=3,
    )
    ax.scatter(
        duck["patient"] + jitter_duck,
        duck["post_surgical_score"],
        s=size,
        marker="^",
        edgecolor="white",
        linewidth=0.5,
        color=duck_color,
        alpha=alpha,
        label="Doc Duck" if size == 70 else None,
        zorder=3,
    )

mean_dreamy = dreamy["post_surgical_score"].mean()
mean_duck   = duck["post_surgical_score"].mean()

ax.axhline(mean_dreamy, linestyle="--", color=dreamy_color, linewidth=2, alpha=0.9)
ax.axhline(mean_duck,   linestyle="--", color=duck_color,   linewidth=2, alpha=0.9)

# Place mean labels inside the plotting area
x_center = n_patients_obs * 0.7
ax.text(
    x_center,
    mean_dreamy,
    f"○ Doc Dreamy mean = {mean_dreamy:.2f}",
    color=dreamy_color,
    fontsize=10,
    va="bottom",
    ha="left",
)
ax.text(
    x_center,
    mean_duck,
    f"△ Doc Duck mean = {mean_duck:.2f}",
    color=duck_color,
    fontsize=10,
    va="top",
    ha="left",
)

ax.set_xlabel("Patient number", color="white", fontsize=11)
ax.set_ylabel("Post-surgical symptom score (lower is better)", color="white", fontsize=11)
ax.set_title("Post-surgical symptom scores – observational data", color="white", fontsize=13, pad=10)

ax.grid(True, linestyle=":", color=grid_color, alpha=0.6)
ax.set_xlim(0, n_patients_obs + 1)

# Style tick labels and legend
ax.tick_params(colors="white")
legend = ax.legend(frameon=False, loc="upper right")
for text in legend.get_texts():
    text.set_color("white")

plt.tight_layout()
plt.show()
```

Figure&nbsp;1 reinforces the first impression from the table: the horizontal mean line for Doc Dreamy lies below the mean line for Doc Duck, suggesting better outcomes for Doc Dreamy when we look only at aggregate observational data.

From a causal perspective, however, this is an incomplete story. We still need to understand **why** patients ended up with each surgeon in the first place.

## Severity as a Common-Cause Confounder

A useful way to structure this problem is through directed acyclic graphs (DAGs). A simple mental model is shown in Figure&nbsp;2.

![Simple model: surgeon directly affects outcome.](dag-example-node.png){#fig-example-node width=55%}

In this initial picture, surgeon identity directly affects the post-surgical outcome. If we believe this model and take the observational difference in mean symptom scores at face value, we might conclude that Doc Dreamy is genuinely better.

However, suppose baseline severity influences both **which surgeon a patient sees** and **how well that patient recovers**. In that case, severity becomes a common-cause confounder, as illustrated in Figure&nbsp;3.

![Common-cause model: severity influences both surgeon choice and outcome.](dag-example2-node.png){#fig-example2-node width=55%}

A plausible story is the following:

- Patients with **milder symptoms** may be willing to wait longer and may prefer the surgeon who appears more polished and reassuring (Doc Dreamy).
- Patients with **more severe symptoms** may be motivated to seek the earliest available surgery date, even if that means seeing a surgeon who does not fit the stereotypical image (Doc Duck).

If this is true, then Doc Dreamy will systematically see easier cases and Doc Duck will systematically see harder cases. In this setting, the raw comparison of mean outcomes is biased in favor of Doc Dreamy, even if Doc Duck is actually the more skilled surgeon.

To recover a credible causal comparison, we need a design that breaks the link between severity and surgeon choice, or a statistical strategy that accounts for severity directly.

## Randomized Assignment: Breaking the Confounding Path

Randomized experiments are designed to address precisely this type of problem. Under randomization, patients no longer choose their surgeon based on appearance, availability, or severity. Instead, a random mechanism (like a coin flip) assigns each patient to a surgeon, making surgeon identity independent of baseline severity.

This design is summarized conceptually in Figure&nbsp;4.

![Randomization breaks the link between severity and surgeon choice.](dag-example3-node.png){#fig-example3-node width=65%}

In this DAG, severity still affects the outcome, but it no longer affects surgeon choice. Randomization determines the surgeon, which allows us to interpret differences in outcomes as the causal effect of surgeon identity, up to random variation.

### Summary of Randomized Outcomes

```{python}
#| label: tbl-randomized-summary

summary_rnd = (
    patients_randomized_df
    .groupby("doctor_name")
    .agg(
        n=("post_surgical_score", "size"),
        mean_score=("post_surgical_score", "mean"),
        std_score=("post_surgical_score", "std"),
        mean_severity=("severity", "mean"),
    )
    .reset_index()
)

style_summary(summary_rnd)
```

The randomized summary reveals a very different pattern from the observational data. In this setting:

- Doc Duck has a **lower average post-surgical score** than Doc Dreamy.
- The average baseline severity is similar across the two surgeons, reflecting the fact that assignment was not driven by severity.

Because surgeon identity is no longer entangled with baseline severity, this difference in mean outcomes is much closer to the causal effect of interest.

### Figure&nbsp;2 – Randomized Outcomes by Patient

```{python}
#| label: fig-randomized
#| fig-cap: "Post-surgical symptom scores under randomized surgeon assignment."
#| out-width: "100%"
#| fig-align: "center"

fig, ax = plt.subplots(figsize=(7, 4), facecolor=bg_color)
ax.set_facecolor(bg_color)

dreamy_r = patients_randomized_df[patients_randomized_df["doctor_name"] == "Doc Dreamy"]
duck_r   = patients_randomized_df[patients_randomized_df["doctor_name"] == "Doc Duck"]

rng = np.random.default_rng(456)
jitter_dreamy_r = rng.uniform(-0.15, 0.15, size=len(dreamy_r))
jitter_duck_r   = rng.uniform(-0.15, 0.15, size=len(duck_r))

for size, alpha in [(70, 0.9), (120, 0.15)]:
    ax.scatter(
        dreamy_r["patient"] + jitter_dreamy_r,
        dreamy_r["post_surgical_score"],
        s=size,
        marker="o",
        edgecolor="white",
        linewidth=0.5,
        color=dreamy_color,
        alpha=alpha,
        label="Doc Dreamy" if size == 70 else None,
        zorder=3,
    )
    ax.scatter(
        duck_r["patient"] + jitter_duck_r,
        duck_r["post_surgical_score"],
        s=size,
        marker="^",
        edgecolor="white",
        linewidth=0.5,
        color=duck_color,
        alpha=alpha,
        label="Doc Duck" if size == 70 else None,
        zorder=3,
    )

mean_dreamy_r = dreamy_r["post_surgical_score"].mean()
mean_duck_r   = duck_r["post_surgical_score"].mean()

ax.axhline(mean_dreamy_r, linestyle="--", color=dreamy_color, linewidth=2, alpha=0.9)
ax.axhline(mean_duck_r,   linestyle="--", color=duck_color, linewidth=2, alpha=0.9)

x_center_r = n_patients_rnd * 0.7
ax.text(
    x_center_r,
    mean_dreamy_r,
    f"○ Doc Dreamy mean = {mean_dreamy_r:.2f}",
    color=dreamy_color,
    fontsize=10,
    va="bottom",
    ha="left",
)
ax.text(
    x_center_r,
    mean_duck_r,
    f"△ Doc Duck mean = {mean_duck_r:.2f}",
    color=duck_color,
    fontsize=10,
    va="top",
    ha="left",
)

ax.set_xlabel("Patient number", color="white", fontsize=11)
ax.set_ylabel("Post-surgical symptom score (lower is better)", color="white", fontsize=11)
ax.set_title("Post-surgical symptom scores – randomized assignment", color="white", fontsize=13, pad=10)

ax.grid(True, linestyle=":", color=grid_color, alpha=0.6)
ax.set_xlim(0, n_patients_rnd + 1)

ax.tick_params(colors="white")
legend = ax.legend(frameon=False, loc="upper right")
for text in legend.get_texts():
    text.set_color("white")

plt.tight_layout()
plt.show()
```

In Figure&nbsp;2, the mean line for Doc Duck lies below the mean line for Doc Dreamy. With assignment randomized, this provides evidence that Doc Duck is the more effective surgeon on average, even though the observational data initially suggested the opposite.

Randomization allows us to ask a clean question:

> *If two similar groups of patients are assigned to surgeons by chance, which surgeon tends to achieve better outcomes?*

In these data, the randomized comparison points to Doc Duck.

## Stratification by Severity: When Randomization Is Not Available

Randomized experiments are powerful, but they are not always feasible. In many real-world settings, we must work with observational data only. In those cases, one common strategy is **stratification**: comparing outcomes within groups of patients who share similar values of the confounder.

Here, the confounder is baseline severity. We therefore examine post-surgical scores as a function of initial severity, using the observational data.

### Figure&nbsp;3 – Outcomes by Baseline Severity

```{python}
#| label: fig-severity
#| fig-cap: "Post-surgical symptom scores versus baseline severity, highlighting the region where both surgeons treat many patients."
#| out-width: "100%"
#| fig-align: "center"

fig, ax = plt.subplots(figsize=(7, 4), facecolor=bg_color)
ax.set_facecolor(bg_color)

dreamy = patients_df[patients_df["doctor_name"] == "Doc Dreamy"]
duck   = patients_df[patients_df["doctor_name"] == "Doc Duck"]

for size, alpha in [(70, 0.9), (120, 0.15)]:
    ax.scatter(
        dreamy["severity"],
        dreamy["post_surgical_score"],
        s=size,
        marker="o",
        edgecolor="white",
        linewidth=0.5,
        color=dreamy_color,
        alpha=alpha,
        label="Doc Dreamy" if size == 70 else None,
        zorder=3,
    )
    ax.scatter(
        duck["severity"],
        duck["post_surgical_score"],
        s=size,
        marker="^",
        edgecolor="white",
        linewidth=0.5,
        color=duck_color,
        alpha=alpha,
        label="Doc Duck" if size == 70 else None,
        zorder=3,
    )

# Highlight the region with overlapping severity values
ax.axvspan(-1, 1, color="#6B7280", alpha=0.18, zorder=1)

mean_dreamy_obs = dreamy["post_surgical_score"].mean()
mean_duck_obs   = duck["post_surgical_score"].mean()

ax.axhline(mean_dreamy_obs, linestyle="--", color=dreamy_color, linewidth=2, alpha=0.9)
ax.axhline(mean_duck_obs,   linestyle="--", color=duck_color, linewidth=2, alpha=0.9)

x_for_text = patients_df["severity"].max() - 0.5

ax.text(
    x_for_text,
    mean_dreamy_obs,
    f"○ Doc Dreamy mean = {mean_dreamy_obs:.2f}",
    color=dreamy_color,
    fontsize=10,
    va="bottom",
    ha="right",
)
ax.text(
    x_for_text,
    mean_duck_obs,
    f"△ Doc Duck mean = {mean_duck_obs:.2f}",
    color=duck_color,
    fontsize=10,
    va="top",
    ha="right",
)

ax.set_xlabel("Baseline severity", color="white", fontsize=11)
ax.set_ylabel("Post-surgical symptom score (lower is better)", color="white", fontsize=11)
ax.set_title("Post-surgical symptom scores by baseline severity – observational data", color="white", fontsize=13, pad=10)

ax.grid(True, linestyle=":", color=grid_color, alpha=0.6)
ax.tick_params(colors="white")

legend = ax.legend(frameon=False, loc="upper right")
for text in legend.get_texts():
    text.set_color("white")

plt.tight_layout()
plt.show()
```

In Figure&nbsp;3, the horizontal axis represents baseline severity and the vertical axis represents post-surgical scores. The shaded band between –1 and 1 marks a region of **overlap**, where both surgeons treat a substantial number of patients with similar initial severity. Outside this band, Doc Duck tends to see more severe patients (higher severity values), while Doc Dreamy sees more mild cases (lower severity values).

### Summary Within the Overlap Band

```{python}
#| label: tbl-overlap-summary

overlap = patients_df[(patients_df["severity"] >= -1) & (patients_df["severity"] <= 1)]

summary_overlap = (
    overlap
    .groupby("doctor_name")
    .agg(
        n=("post_surgical_score", "size"),
        mean_score=("post_surgical_score", "mean"),
        std_score=("post_surgical_score", "std"),
        mean_severity=("severity", "mean"),
    )
    .reset_index()
)

style_summary(summary_overlap)
```

The overlap summary focuses on patients whose baseline severity lies between –1 and 1, where the two surgeons are directly comparable. Within this band, the difference in average post-surgical scores is smaller than in the full observational sample, and Doc Duck often appears more competitive or even favorable relative to Doc Dreamy.

This stratified comparison reduces the bias introduced by severity. While it does not replicate the clean separation achieved by randomization, it moves the analysis closer to an apples-to-apples comparison.

## Discussion and Main Lessons

Taken together, the observational analysis, randomized experiment, and stratified comparison tell a consistent story about confounding and causal interpretation.

1. **Raw observational averages can be misleading.**  
   In the observational data, Doc Dreamy initially appears to be the better surgeon because his patients have lower average post-surgical scores. However, he also tends to see less severe cases, while Doc Duck treats more severe cases. The naive comparison of means therefore conflates surgeon skill with differences in case mix.

2. **Randomization breaks the link between severity and treatment.**  
   In the randomized data, surgeon assignment is no longer determined by factors such as severity or perceived appearance. Under this design, Doc Duck achieves lower average post-surgical scores than Doc Dreamy, suggesting that he is the more effective surgeon when patients are comparable at baseline.

3. **Stratification is a practical alternative when randomization is not available.**  
   When only observational data are available, stratifying on baseline severity helps to control for the confounder. Comparing outcomes within bands of severity (such as the overlap region between –1 and 1) produces a fairer comparison and reduces the bias induced by non-random patient allocation.

The broader message is that **causal interpretation requires more than a simple comparison of averages**. Understanding the data-generating process—who sees which surgeon and why—is essential. Randomization, when feasible, is a powerful design tool for breaking confounding pathways. When randomization is not possible, careful adjustment strategies such as stratification can still provide meaningful insight, though with stronger assumptions.

In this example, the combination of randomized evidence and stratified observational analysis points to a clear conclusion: despite appearances and initial impressions from the raw observational data, Doc Duck is likely the better surgeon. The apparent advantage of Doc Dreamy in the observational analysis is largely explained by severity acting as a common-cause confounder.

