
---
title: "Using Randomization & Stratification to Overcome a Common Cause Confounder"
subtitle: "Comparing Surgical Outcomes for Doc Dreamy and Doc Duck"
format:
  html:
    toc: true
    toc-depth: 2
    number-sections: true
    fig-align: center
    fig-width: 7
    fig-height: 4
execute:
  echo: false
  eval: true
  warning: false
  message: false
---

```{=html}
<style>
.quarto-figure img, img {
  max-width: 100%;
  height: auto;
}
</style>
```

## Introduction

Clinical data often look convincing at first glance. A simple comparison of averages can make one treatment or clinician appear superior. However, when treatment choice is related to patient characteristics, those same averages can be badly misleading.

In this report, we study outcomes for two surgeons working at the same hospital:

- **Doc Dreamy**, who fits the stereotypical image of a highly polished specialist.
- **Doc Duck**, whose appearance and style may not match common expectations.

```{=html}
<div style="display:flex; justify-content:center; margin: 1rem 0 1.5rem 0;">
  <img src="docSideBySide.jpg"
       alt="Doc Dreamy and Doc Duck"
       style="max-width: 340px; border-radius: 10px; box-shadow: 0 0 18px rgba(0,0,0,0.25);">
</div>
```

We use observational data, randomized data, and stratification by baseline severity to show how a common-cause confounder can reverse the apparent ranking of these two surgeons. Randomization and stratification are then used to recover a more credible causal story.

## Data and Notation

```{python}
#| label: load-data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load observational and randomized data from CSV
patients_df = pd.read_csv("patients_data.csv")
patients_randomized_df = pd.read_csv("patients_data_randomized.csv")

# Color choices (prof-specified, colorblind-friendly)
dreamy_color = "#4E79A7"  # blue
duck_color   = "#E15759"  # coral / red-orange
grid_color   = "#D1D5DB"  # light gray grid

n_patients_obs = len(patients_df)
n_patients_rnd = len(patients_randomized_df)
```

We work with two data sets:

- **Observational data**: patients selected their surgeon under usual care (driven by preferences, urgency, appearance, scheduling, and so on).
- **Randomized data**: patients were assigned to a surgeon using a random mechanism, independent of patient characteristics.

For each patient, we observe:

- `severity` – baseline symptom severity before surgery.
- `doctor_name` – `"Doc Dreamy"` or `"Doc Duck"`.
- `post_surgical_score` – symptom score after surgery  
  (lower values indicate better outcomes).

Throughout, the goal is not only to describe these data, but to understand **how much of the observed difference in outcomes can be attributed to the choice of surgeon**, rather than to differences in the types of patients they treat.

## Observational Data: A First, Misleading Impression

In the observational dataset, patients did not receive surgeons at random. They chose, or were scheduled with, a surgeon based on real-world factors such as availability, perceived skill, and appearance. As a result, baseline severity can be imbalanced between the two surgeons.

### Summary of Observational Outcomes

```{python}
#| label: tbl-observational-summary

summary_obs = (
    patients_df
    .groupby("doctor_name")
    .agg(
        n=("post_surgical_score", "size"),
        mean_score=("post_surgical_score", "mean"),
        std_score=("post_surgical_score", "std"),
        mean_severity=("severity", "mean"),
    )
    .reset_index()
)

def style_summary(df, caption):
    # Colorful summary table:
    # - Blue pill for Doc Dreamy mean_score
    # - Coral pill for Doc Duck mean_score
    def kpi_style(row):
        styles = [""] * len(row)
        if "mean_score" in row.index:
            idx = list(row.index).index("mean_score")
            if row["doctor_name"] == "Doc Dreamy":
                styles[idx] = (
                    "background-color: #4E79A7; color: white; "
                    "border-radius: 999px; padding: 4px 10px;"
                )
            elif row["doctor_name"] == "Doc Duck":
                styles[idx] = (
                    "background-color: #E15759; color: white; "
                    "border-radius: 999px; padding: 4px 10px;"
                )
        return styles

    styler = (
        df.style
        .set_caption(caption)
        .format({
            "mean_score": "{:.2f}",
            "std_score": "{:.2f}",
            "mean_severity": "{:.2f}",
        })
        .set_table_attributes('class="table table-striped table-bordered"')
        .set_properties(**{"text-align": "center"})
        .set_table_styles([
            {"selector": "caption",
             "props": [("caption-side", "top"),
                       ("font-weight", "bold"),
                       ("padding", "6px"),
                       ("color", "#111827"),
                       ("font-size", "0.98rem")]},
            {"selector": "th",
             "props": [("background-color", "#EEF2FF"),
                       ("color", "#111827"),
                       ("padding", "8px"),
                       ("border-bottom", "2px solid #C7D2FE")]},
            {"selector": "td",
             "props": [("padding", "6px"),
                       ("color", "#111827"),
                       ("border-top", "1px solid #E5E7EB")]},
        ])
    )
    styler = styler.apply(kpi_style, axis=1)
    return styler

style_summary(
    summary_obs,
    "Table 1. Summary of observational outcomes by surgeon."
)
```

In the observational data, Doc Dreamy has a lower average post-surgical symptom score than Doc Duck. Numerically, Dreamy’s mean is **2.80**, compared with **3.38** for Duck—a gap of about **0.6 points** in favor of Doc Dreamy. However, the same summary also shows that Doc Dreamy tends to treat less severe cases on average (mean severity –0.67), while Doc Duck treats more severe cases (mean severity 0.85). This raises the possibility that the apparent advantage for Doc Dreamy is driven by differences in the patients, not differences in skill.

### Observational Outcomes by Patient

```{python}
#| label: fig-observational
#| fig-cap: "Post-surgical symptom score by patient and doctor – observational data."
#| out-width: "100%"
#| fig-align: "center"

dreamy = patients_df[patients_df["doctor_name"] == "Doc Dreamy"]
duck   = patients_df[patients_df["doctor_name"] == "Doc Duck"]

fig, ax = plt.subplots(figsize=(7, 4))

# Scatter points by patient number
ax.scatter(
    dreamy["patient"],
    dreamy["post_surgical_score"],
    s=40,
    color=dreamy_color,
    marker="o",
    edgecolor="black",
    linewidth=0.4,
    alpha=0.8,
    label="Doc Dreamy (○)",
)
ax.scatter(
    duck["patient"],
    duck["post_surgical_score"],
    s=40,
    color=duck_color,
    marker="^",
    edgecolor="black",
    linewidth=0.4,
    alpha=0.8,
    label="Doc Duck (△)",
)

# Mean lines
mean_dreamy = dreamy["post_surgical_score"].mean()
mean_duck   = duck["post_surgical_score"].mean()

ax.axhline(mean_dreamy, color=dreamy_color, linestyle="--", linewidth=2)
ax.axhline(mean_duck,   color=duck_color,   linestyle="--", linewidth=2)

# Text annotations near the right side to avoid overlap
x_max = patients_df["patient"].max()
x_text = x_max + 2

ax.text(
    x_text,
    mean_dreamy,
    f"○ Doc Dreamy mean = {mean_dreamy:.2f}",
    color=dreamy_color,
    va="center",
)
ax.text(
    x_text,
    mean_duck,
    f"△ Doc Duck mean = {mean_duck:.2f}",
    color=duck_color,
    va="center",
)

ax.set_xlim(1, x_text + 3)

ax.set_xlabel("Patient number")
ax.set_ylabel("Post-surgical symptom score (lower is better)")
ax.set_title("Post-Surgical Symptom Score by Patient and Doctor")

ax.grid(True, linestyle=":", color=grid_color)
ax.legend(frameon=False, loc="upper left")

plt.tight_layout()
plt.show()
```

From this figure we see the post-surgical scores for all 100 observational patients by surgeon, along with dashed mean lines and clearly separated annotations for the average outcomes.

## Severity as a Common-Cause Confounder

A useful way to structure this problem is through directed acyclic graphs (DAGs). A simple mental model is shown in Figure 2.

![Simple model: surgeon directly affects outcome.](dag-example-node.png){#fig-example-node width=55%}

In this initial picture, surgeon identity directly affects the post-surgical outcome. If we believe this model and take the observational difference in mean symptom scores at face value, we might conclude that Doc Dreamy is genuinely better.

However, suppose baseline severity influences both **which surgeon a patient sees** and **how well that patient recovers**. In that case, severity becomes a common-cause confounder, as illustrated in Figure 3.

![Common-cause model: severity influences both surgeon choice and outcome.](dag-example2-node.png){#fig-example2-node width=55%}

A plausible story is the following:

- Patients with **milder symptoms** may be willing to wait longer and may prefer the surgeon who appears more polished and reassuring (Doc Dreamy).
- Patients with **more severe symptoms** may be motivated to seek the earliest available surgery date, even if that means seeing a surgeon who does not fit the stereotypical image (Doc Duck).

If this is true, then Doc Dreamy will systematically see easier cases and Doc Duck will systematically see harder cases. In this setting, the raw comparison of mean outcomes is biased in favor of Doc Dreamy, even if Doc Duck is actually the more skilled surgeon.

To recover a credible causal comparison, we need a design that breaks the link between severity and surgeon choice, or a statistical strategy that accounts for severity directly.

## Randomized Assignment: Breaking the Confounding Path

Randomized experiments are designed to address precisely this type of problem. Under randomization, patients no longer choose their surgeon based on appearance, availability, or severity. Instead, a random mechanism (like a coin flip) assigns each patient to a surgeon, making surgeon identity independent of baseline severity.

This design is summarized conceptually in Figure 4.

![Randomization breaks the link between severity and surgeon choice.](dag-example3-node.png){#fig-example3-node width=65%}

In this DAG, severity still affects the outcome, but it no longer affects surgeon choice. Randomization determines the surgeon, which allows us to interpret differences in outcomes as the causal effect of surgeon identity, up to random variation.

### Summary of Randomized Outcomes

```{python}
#| label: tbl-randomized-summary

summary_rnd = (
    patients_randomized_df
    .groupby("doctor_name")
    .agg(
        n=("post_surgical_score", "size"),
        mean_score=("post_surgical_score", "mean"),
        std_score=("post_surgical_score", "std"),
        mean_severity=("severity", "mean"),
    )
    .reset_index()
)

style_summary(
    summary_rnd,
    "Table 2. Summary of randomized outcomes by surgeon."
)
```

The randomized summary reveals a very different pattern from the observational data. In this setting:

- Doc Duck has a **lower average post-surgical score** than Doc Dreamy.
- The average baseline severity is similar across the two surgeons (means around 0.0–0.2), reflecting the fact that assignment was not driven by severity.

Numerically, the mean post-surgical score is **3.46** for Doc Dreamy and **2.71** for Doc Duck—a gap of about **0.75 points** in favor of Doc Duck when patients are assigned at random.

### Randomized Outcomes by Patient

```{python}
#| label: fig-randomized
#| fig-cap: "Post-surgical symptom score (randomized assignment) by patient and doctor."
#| out-width: "100%"
#| fig-align: "center"

dreamy_r = patients_randomized_df[patients_randomized_df["doctor_name"] == "Doc Dreamy"]
duck_r   = patients_randomized_df[patients_randomized_df["doctor_name"] == "Doc Duck"]

fig, ax = plt.subplots(figsize=(7, 4))

ax.scatter(
    dreamy_r["patient"],
    dreamy_r["post_surgical_score"],
    s=40,
    color=dreamy_color,
    marker="o",
    edgecolor="black",
    linewidth=0.4,
    alpha=0.8,
    label="Doc Dreamy (○)",
)
ax.scatter(
    duck_r["patient"],
    duck_r["post_surgical_score"],
    s=40,
    color=duck_color,
    marker="^",
    edgecolor="black",
    linewidth=0.4,
    alpha=0.8,
    label="Doc Duck (△)",
)

mean_dreamy_r = dreamy_r["post_surgical_score"].mean()
mean_duck_r   = duck_r["post_surgical_score"].mean()

ax.axhline(mean_dreamy_r, color=dreamy_color, linestyle="--", linewidth=2)
ax.axhline(mean_duck_r,   color=duck_color,   linestyle="--", linewidth=2)

x_max_r = patients_randomized_df["patient"].max()
x_text_r = x_max_r + 2

ax.text(
    x_text_r,
    mean_dreamy_r,
    f"○ Doc Dreamy mean = {mean_dreamy_r:.2f}",
    color=dreamy_color,
    va="center",
)
ax.text(
    x_text_r,
    mean_duck_r,
    f"△ Doc Duck mean = {mean_duck_r:.2f}",
    color=duck_color,
    va="center",
)

ax.set_xlim(1, x_text_r + 3)

ax.set_xlabel("Patient number")
ax.set_ylabel("Post-surgical symptom score (lower is better)")
ax.set_title("Post-Surgical Symptom Score (Randomized Assignment)")

ax.grid(True, linestyle=":", color=grid_color)
ax.legend(frameon=False, loc="upper left")

plt.tight_layout()
plt.show()
```

Under randomization, the dashed mean line for Doc Duck lies clearly below Doc Dreamy’s, and the annotations emphasize that Duck’s average outcome is better when patients are assigned by chance rather than by self-selection.

Randomization allows us to ask a clean question:

> *If two similar groups of patients are assigned to surgeons by chance, which surgeon tends to achieve better outcomes?*

In these data, the randomized comparison points to Doc Duck.

## Stratification by Severity: When Randomization Is Not Available

Randomized experiments are powerful, but they are not always feasible. In many real-world settings, we must work with observational data only. In those cases, one common strategy is **stratification**: comparing outcomes within groups of patients who share similar values of the confounder.

Here, the confounder is baseline severity. We therefore examine post-surgical scores as a function of initial severity, using the observational data.

### Outcomes by Baseline Severity

```{python}
#| label: fig-severity
#| fig-cap: "Post-surgical symptom score by baseline severity – observational data, with overlap region highlighted."
#| out-width: "100%"
#| fig-align: "center"

fig, ax = plt.subplots(figsize=(7, 4))

dreamy = patients_df[patients_df["doctor_name"] == "Doc Dreamy"]
duck   = patients_df[patients_df["doctor_name"] == "Doc Duck"]

# Overlap band
ax.axvspan(-1, 1, color="#E5E7EB", alpha=0.8, label="Overlap region (-1 to 1)")

ax.scatter(
    dreamy["severity"],
    dreamy["post_surgical_score"],
    s=40,
    color=dreamy_color,
    marker="o",
    edgecolor="black",
    linewidth=0.4,
    alpha=0.8,
    label="Doc Dreamy (○)",
)
ax.scatter(
    duck["severity"],
    duck["post_surgical_score"],
    s=40,
    color=duck_color,
    marker="^",
    edgecolor="black",
    linewidth=0.4,
    alpha=0.8,
    label="Doc Duck (△)",
)

# Mean lines (overall observational means)
mean_dreamy_obs = dreamy["post_surgical_score"].mean()
mean_duck_obs   = duck["post_surgical_score"].mean()

ax.axhline(mean_dreamy_obs, color=dreamy_color, linestyle="--", linewidth=2)
ax.axhline(mean_duck_obs,   color=duck_color,   linestyle="--", linewidth=2)

x_right = patients_df["severity"].max() + 0.3

ax.text(
    x_right,
    mean_dreamy_obs,
    f"○ Doc Dreamy mean = {mean_dreamy_obs:.2f}",
    color=dreamy_color,
    va="center",
)
ax.text(
    x_right,
    mean_duck_obs,
    f"△ Doc Duck mean = {mean_duck_obs:.2f}",
    color=duck_color,
    va="center",
)

ax.set_xlabel("Baseline severity")
ax.set_ylabel("Post-surgical symptom score (lower is better)")
ax.set_title("Post-Surgical Symptom Score by Patient Severity")

ax.grid(True, linestyle=":", color=grid_color)
ax.legend(frameon=False, loc="upper left")

plt.tight_layout()
plt.show()
```

In this figure, the horizontal axis represents baseline severity and the vertical axis represents post-surgical scores. The shaded band between –1 and 1 marks a region of **overlap**, where both surgeons treat a substantial number of patients with similar initial severity. Outside this band, Doc Duck tends to see more severe patients (higher severity values), while Doc Dreamy sees more mild cases (lower severity values). The dashed mean lines and annotations summarize the overall outcomes for each surgeon.

### Summary Within the Overlap Band

```{python}
#| label: tbl-overlap-summary

overlap = patients_df[(patients_df["severity"] >= -1) & (patients_df["severity"] <= 1)]

summary_overlap = (
    overlap
    .groupby("doctor_name")
    .agg(
        n=("post_surgical_score", "size"),
        mean_score=("post_surgical_score", "mean"),
        std_score=("post_surgical_score", "std"),
        mean_severity=("severity", "mean"),
    )
    .reset_index()
)

style_summary(
    summary_overlap,
    "Table 3. Summary of outcomes within the overlap band (-1 ≤ severity ≤ 1)."
)
```

The overlap summary focuses on patients whose baseline severity lies between –1 and 1, where the two surgeons are directly comparable. Within this band, the difference in average post-surgical scores is smaller than in the full observational sample, and Doc Duck often appears more competitive or even favorable relative to Doc Dreamy. Here, Duck’s mean score is **2.94** versus **3.23** for Dreamy, reversing the ranking from the full observational sample.

This stratified comparison reduces the bias introduced by severity. While it does not replicate the clean separation achieved by randomization, it moves the analysis closer to an apples-to-apples comparison.

## Discussion and Main Lessons

Taken together, the observational analysis, randomized experiment, and stratified comparison tell a consistent story about confounding and causal interpretation.

1. **Raw observational averages can be misleading.**  
   In the observational data, Doc Dreamy initially appears to be the better surgeon because his patients have lower average post-surgical scores (2.80 vs 3.38). However, he also tends to see less severe cases (mean severity –0.67), while Doc Duck treats more severe cases (0.85). The naive comparison of means therefore conflates surgeon skill with differences in case mix.

2. **Randomization breaks the link between severity and treatment.**  
   In the randomized data, surgeon assignment is no longer determined by factors such as severity or perceived appearance. Under this design, Doc Duck achieves lower average post-surgical scores than Doc Dreamy (2.71 vs 3.46), suggesting that he is the more effective surgeon when patients are comparable at baseline.

3. **Stratification is a practical alternative when randomization is not available.**  
   When only observational data are available, stratifying on baseline severity helps to control for the confounder. Comparing outcomes within bands of severity (such as the overlap region between –1 and 1) produces a fairer comparison and reduces the bias induced by non-random patient allocation. Within this overlap band, Duck’s mean score (2.94) is lower than Dreamy’s (3.23), echoing the story from the randomized experiment.

The broader message is that **causal interpretation requires more than a simple comparison of averages**. Understanding the data-generating process—who sees which surgeon and why—is essential. Randomization, when feasible, is a powerful design tool for breaking confounding pathways. When randomization is not possible, careful adjustment strategies such as stratification can still provide meaningful insight, though with stronger assumptions.

In this example, the combination of randomized evidence and stratified observational analysis points to a clear conclusion: despite appearances and initial impressions from the raw observational data, Doc Duck is likely the better surgeon. The apparent advantage of Doc Dreamy in the observational analysis is largely explained by severity acting as a common-cause confounder.
